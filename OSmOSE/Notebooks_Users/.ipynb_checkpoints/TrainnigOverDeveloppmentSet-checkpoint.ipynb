{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a911ba53-f148-436a-a84b-a107b118cb5e",
   "metadata": {},
   "source": [
    "## Import functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e4e47-d0ea-4958-a22d-da09b20076c7",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">*JUST RUN CELL*</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239dbd34-388b-4dfe-94a7-02fb4184eea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.1\n",
      "Torchvision Version:  0.11.2\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "#os.chdir(os.path.join(\"/home/datawork-osmose/\",'osmoseNotebooks_v0','source'))\n",
    "\n",
    "os.chdir(os.path.join(\"C:/Users/gabri.DESKTOP-QIPER28/Documents/PhD/CodesDatarmor/Functions/FINAL\"))\n",
    "from TrainNetwork import TrainNetwork_main, CheckAvailableAI_DataSplit\n",
    "from launcher_datasetScale import list_datasets\n",
    "from CreateDatasetForTorch import CheckAvailableAI_tasks_BM_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431855bb-1d21-48a5-8756-f29c59125a6f",
   "metadata": {},
   "source": [
    "## Selection of dataset, task, benchmark and datasplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708a79e-3a4c-4afe-afd3-91287115fa62",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">*RUN CELL & FILL*</span>\n",
    "\n",
    "- ``dataset_ID`` : nom du jeu de données à traiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f144bfe-8591-40cf-9404-d13784812eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "  - Glider\n"
     ]
    }
   ],
   "source": [
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525370f9-56b7-417c-b96a-4264205d1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ID = 'Glider'\n",
    "\n",
    "#display_metadata(dataset_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056131f6-da18-41cf-9b7d-d19f6f09012a",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">*RUN CELL & FILL*</span>\n",
    "\n",
    "It will print all the folder in the AI branch.\n",
    "\n",
    "- `` Task_ID `` : The First Level is for the Task.    (nb : for now, only 'Task1_DetWeakLabel' available)\n",
    "- ``BM_Name`` : The Second Level is for the BenchMark - . A BenchMark is composed by different model that will be compared. (Different architecture, different dataset subdivision, differents representation, ...)\n",
    "- The Third Level is for the dataset used - same notation than the folder that contains specrograms : \"analysis_sf\"_\"LengthFile\"\n",
    "    - ``analysis_fs`` : Fréquence d'échantillonnage des fichier à traiter (en Hz)\n",
    "    - ``LengthFile`` : Durée (en secondes) des fichiers à traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad27eed-49bd-4cd8-a7d9-267b34d3e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "Task1_DetWeakLabel/\n",
      "    DetWL_BlueWhaleAus/\n",
      "        50_500/\n"
     ]
    }
   ],
   "source": [
    "CheckAvailableAI_tasks_BM_model(dataset_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5792cb-bea1-4e92-a3cd-4b59bbc96bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Task_ID :\n",
    "Task_ID = 'Task1_DetWeakLabel'\n",
    "# Select BM_Name :\n",
    "BM_Name = 'DetWL_BlueWhaleAus'\n",
    "\n",
    "# Select analysis_fs and LengthFile :\n",
    "analysis_fs = 500 #Hz\n",
    "LengthFile = 50 #s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3e2d1-888a-4f34-8578-f8173b54ced3",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">*RUN CELL & FILL*</span>\n",
    "\n",
    "It will print all the existing datasplit\n",
    "\n",
    "- `` SplitName `` : Enter the one you want to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06fdf8b8-e239-4bd2-8ba9-a6fcb08b9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info_datasplit/\n",
      "    FullyRandom_v1/\n"
     ]
    }
   ],
   "source": [
    "CheckAvailableAI_DataSplit(dataset_ID, Task_ID, BM_Name, LengthFile, analysis_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87834994-73b3-4897-8045-0a5abf48fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitName = 'FullyRandom_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c57efe-4eec-4932-897b-71b4ba32d370",
   "metadata": {},
   "source": [
    "## Definition of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63696557-08df-40a4-8e2d-6ed2f2912ad1",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">*FILL & RUN CELL*</span>\n",
    "\n",
    "- `` Version_name `` : Choose a name for your model \n",
    "\n",
    "- `` ModelName `` : Architecture of the model. Please, select one in this list : (for more details, check : https://pytorch.org/vision/main/models.html)\n",
    "    - 'resnet18'\n",
    "    - 'resnet50'\n",
    "    - 'resnet101'\n",
    "    - 'vgg11'\n",
    "    - 'vgg11_bn'\n",
    "    - 'vgg13'\n",
    "    - 'vgg13_bn'\n",
    "    - 'vgg19'\n",
    "    - 'vgg19_bn'\n",
    "    - 'alexnet'\n",
    "    - 'squeezenet'\n",
    "    - 'densenet'\n",
    "    - 'inception'\n",
    "    \n",
    "    \n",
    "- `` use_pretrained `` : (for more details, check : https://pytorch.org/vision/main/models.html) (By default : use_pretrained = True)\n",
    "    - True : if you want to used already pretrained network one reference image dataset and just finetun the last layer \n",
    "    - False : if you want to train your network from random weights and adjust all layers\n",
    "\n",
    "\n",
    "       \n",
    "- `` TrainSetRatio `` : ratio between 0 and 1 of all the developpement set that will be used for the training (the oser part is for testing). (if None : TrainSetRatio = 0.9)\n",
    "\n",
    "- `` batch_size `` : Number of spectrograms per batch (if None : batch_size = 10)\n",
    "- `` learning_rate `` : step size at each iteration while moving toward a minimum of a loss function (if None : learning_rate = 1e-3)\n",
    "- `` num_epochs `` : Number of iteration over all developpement set (if None : num_epochs = 10)\n",
    "\n",
    "- `` Dyn `` : Numpy array with two elements np.array([LevelMindB,LevelMaxdB]). Define the lower and upper level threshold apply on the spectrograms. If Dyn=None, we will use the dynamic applied for the visualisation. (By default : Dyn = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c691bc36-d890-4df0-ab64-c91e337f44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Version_name = 'V2'\n",
    "\n",
    "ModelName = 'vgg11_bn'\n",
    "use_pretrained = True\n",
    "\n",
    "TrainSetRatio = 0.9\n",
    "\n",
    "batch_size = 10\n",
    "learning_rate = 5e-4 \n",
    "num_epochs = 2\n",
    "\n",
    "Dyn = np.array([-20,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b63647e-9276-4c1f-9266-3e921e817724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INFORMATION : \n",
      " \n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=4096, out_features=1, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/PhD/OSMOSE_TYPE/dataset/Glider\\\\AI\\\\Task1_DetWeakLabel\\\\DetWL_BlueWhaleAus\\\\50_500\\\\info_datasplit\\\\DEVannotations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-617b275abd1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTrainNetwork_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_ID\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTask_ID\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTask_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBM_Name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBM_Name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLengthFile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLengthFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manalysis_fs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVersion_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVersion_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModelName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_pretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_pretrained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainSetRatio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrainSetRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDyn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDyn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\PhD\\CodesDatarmor\\Functions\\FINAL\\TrainNetwork.py\u001b[0m in \u001b[0;36mTrainNetwork_main\u001b[1;34m(dataset_ID, Task_ID, BM_Name, LengthFile, Fs, Version_name, ModelName, use_pretrained, TrainSetRatio, batch_size, learning_rate, num_epochs, Dyn)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;31m#%% Import DEV Annotation, Set Variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[0mtrain_df_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'AI'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTask_ID\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBM_Name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfolderName_audioFiles\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'info_datasplit'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;34m'DEVannotations.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[0mNbFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/PhD/OSMOSE_TYPE/dataset/Glider\\\\AI\\\\Task1_DetWeakLabel\\\\DetWL_BlueWhaleAus\\\\50_500\\\\info_datasplit\\\\DEVannotations.csv'"
     ]
    }
   ],
   "source": [
    "TrainNetwork_main(dataset_ID=dataset_ID, Task_ID=Task_ID, BM_Name=BM_Name, LengthFile=LengthFile, Fs=analysis_fs, SplitName=SplitNamr, Version_name=Version_name, ModelName=ModelName, use_pretrained=use_pretrained, TrainSetRatio=TrainSetRatio, batch_size = batch_size, learning_rate=learning_rate, num_epochs=num_epochs, Dyn=Dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb1576-53a5-4c62-9b80-ab8acb0db7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455bd82-6454-4fba-a7f4-9086e4a4f438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
